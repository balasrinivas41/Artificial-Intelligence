{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suBex3vnr5GO"
      },
      "source": [
        "# Hello, Numerai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7VRkpptr5GR"
      },
      "source": [
        "Hello and welcome to the Numerai Data Science Tournament!\n",
        "\n",
        "This notebook is designed to help you build your first machine learning model and start competing the tournament.\n",
        "\n",
        "In this notebook we will\n",
        "1. Download and explore the Numerai dataset\n",
        "2. Train and evaluate your first machine learning model\n",
        "3. Deploy your model to start making live submissions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THMEU_T4r5GS",
        "outputId": "03622a6e-6015-4874-9833-fde774b9fbf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.1/34.1 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "distributed 2024.12.1 requires cloudpickle>=3.0.0, but you have cloudpickle 2.2.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "cvxpy 1.6.5 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.10.1 which is incompatible.\n",
            "jax 0.5.2 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
            "dask 2024.12.1 requires cloudpickle>=3.0.0, but you have cloudpickle 2.2.1 which is incompatible.\n",
            "jaxlib 0.5.1 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install -q numerapi pandas pyarrow matplotlib lightgbm scikit-learn cloudpickle==2.2.1 scipy==1.10.1\n",
        "\n",
        "# Inline plots\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoeLNZTSr5GT"
      },
      "source": [
        "## 1. Dataset  \n",
        "\n",
        "At a high level, the Numerai dataset is a tabular dataset that describes the stock market over time. It is compiled from high-quality (and expensive) data that might be difficult for individuals to obtain.\n",
        "\n",
        "The unique thing about Numerai's dataset is that it is `obfuscated`, which means that the underlying stock ids, feature names, and target definitions are anonymized. This makes it so that we can give this data out for free and so that it can be modeled without any financial domain knowledge (or bias!)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ECU5uszr5GU"
      },
      "source": [
        "### Listing the datasets\n",
        "Firstly, take a look at the files Numerai offers below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "4B4bbH07r5GU",
        "outputId": "50f8bdb7-4637-47fa-893e-e3b60b51ac62"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ed06a33c0fe9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initialize NumerAPI - the official Python API client for Numerai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumerapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNumerAPI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNumerAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# list the datasets and available versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numerapi/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# pylint: disable=wrong-import-position\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumerapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumerapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNumerAPI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumerapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignalsapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSignalsAPI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumerapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcryptoapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCryptoAPI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numerapi/numerapi.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumerapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumerapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase_api\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numerapi/base_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpytz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     ) from _err\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m from pandas._config import (\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mget_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mset_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_config/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"warn_copy_on_write\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m ]\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdates\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport]  # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m from pandas._config.config import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m from pandas._typing import (\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_typing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBitGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpublic_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ],
      "source": [
        "# Initialize NumerAPI - the official Python API client for Numerai\n",
        "from numerapi import NumerAPI\n",
        "napi = NumerAPI()\n",
        "\n",
        "# list the datasets and available versions\n",
        "all_datasets = napi.list_datasets()\n",
        "dataset_versions = list(set(d.split('/')[0] for d in all_datasets))\n",
        "print(\"Available versions:\\n\", dataset_versions)\n",
        "\n",
        "# Set data version to one of the latest datasets\n",
        "DATA_VERSION = \"v5.0\"\n",
        "\n",
        "# Print all files available for download for our version\n",
        "current_version_files = [f for f in all_datasets if f.startswith(DATA_VERSION)]\n",
        "print(\"Available\", DATA_VERSION, \"files:\\n\", current_version_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avIeXI5T9CC-"
      },
      "source": [
        "### Downloading datasets\n",
        "\n",
        "The `features.json` file contains metadata about features in the dataset including:\n",
        "- statistics on each feature\n",
        "- helpful sets of features\n",
        "- the targets available for training\n",
        "\n",
        "Let's download it and take a look:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Mldufeo9BKS"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# download the feature metadata file\n",
        "napi.download_dataset(f\"{DATA_VERSION}/features.json\")\n",
        "\n",
        "# read the metadata and display\n",
        "feature_metadata = json.load(open(f\"{DATA_VERSION}/features.json\"))\n",
        "for metadata in feature_metadata:\n",
        "  print(metadata, len(feature_metadata[metadata]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sESS7Wfs_pqz"
      },
      "source": [
        "### Feature Sets & Groups\n",
        "As you can see there are many features and targets to choose from.\n",
        "\n",
        "Instead of training a model on all 2000+ features, let's pick a subset of features to analyze.\n",
        "\n",
        "Here are a few starter sets Numerai offers:\n",
        "\n",
        "- `small` contains a minimal subset of features that have the highest [feature importance](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html)\n",
        "\n",
        "- `medium` contains all the \"basic\" features, each unique in some way (e.g. P/E ratios vs analyst ratings)\n",
        "\n",
        "- `all` contains all features in `medium` and their variants (e.g. P/E by country vs P/E by sector)\n",
        "\n",
        "Let's take a look at the medium feature set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeAzyU9q_dwR"
      },
      "outputs": [],
      "source": [
        "feature_sets = feature_metadata[\"feature_sets\"]\n",
        "for feature_set in [\"small\", \"medium\", \"all\"]:\n",
        "  print(feature_set, len(feature_sets[feature_set]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0-iMpdJAubs"
      },
      "source": [
        "\n",
        "The `medium` set seems much more reasonable.\n",
        "\n",
        "Using it will speed up model training and reduce memory usage (required for Colab free tier).\n",
        "\n",
        "Let's load the training data for just the medium feature set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UC5YkX1xr5GV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define our feature set\n",
        "feature_set = feature_sets[\"small\"]\n",
        "# use \"medium\" or \"all\" for better performance. Requires more RAM.\n",
        "# features = feature_metadata[\"feature_sets\"][\"medium\"]\n",
        "# features = feature_metadata[\"feature_sets\"][\"all\"]\n",
        "\n",
        "# Download the training data - this will take a few minutes\n",
        "napi.download_dataset(f\"{DATA_VERSION}/train.parquet\")\n",
        "\n",
        "# Load only the \"medium\" feature set to\n",
        "# Use the \"all\" feature set to use all features\n",
        "train = pd.read_parquet(\n",
        "    f\"{DATA_VERSION}/train.parquet\",\n",
        "    columns=[\"era\", \"target\"] + feature_set\n",
        ")\n",
        "\n",
        "# Downsample to every 4th era to reduce memory usage and speedup model training (suggested for Colab free tier)\n",
        "# Comment out the line below to use all the data\n",
        "train = train[train[\"era\"].isin(train[\"era\"].unique()[::4])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBcMKMX6FoNl"
      },
      "source": [
        "\n",
        "### Training data\n",
        "\n",
        "Each row represents a stock at a specific point in time:\n",
        "- `id` is the stock id\n",
        "- `era` is the date\n",
        "- `target` is a measure of future returns for that stock\n",
        "- `features` describe the attributes of the stock (eg. P/E ratio) for that date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9JOOMqpFscM"
      },
      "outputs": [],
      "source": [
        "train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSvdym9wr5GW"
      },
      "source": [
        "### Eras\n",
        "As mentioned above, each `era` corresponds to a different date. Each era is exactly 1 week apart.\n",
        "\n",
        "It is helpful to think about rows of stocks within the same `era` as a single example. You will notice that throughout this notebook and other examples, we often talk about things \"per era\". For example, the number of rows per era represents the number of stocks in Numerai's investable universe on that date."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JX0Bs95r5GX"
      },
      "outputs": [],
      "source": [
        "# Plot the number of rows per era\n",
        "train.groupby(\"era\").size().plot(\n",
        "    title=\"Number of rows per era\",\n",
        "    figsize=(5, 3),\n",
        "    xlabel=\"Era\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxQCUEEPr5GZ"
      },
      "source": [
        "### Target\n",
        "The `target` is a measure of stock market returns over the next 20 (business) days. Specifically, it is a measure of \"stock-specific\" returns that are not explained by well-known \"factors\" or broader trends in the market, country, or sector. For example, if Apple went up and the tech sector also went up, we only want to know if Apple went up more or less than the tech sector.\n",
        "\n",
        "Target values are binned into 5 unequal bins: `0`, `0.25`, `0.5`, `0.75`, `1.0`. Again, this heavy regularization of target values is to avoid overfitting as the underlying values are extremely noisy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ALp0YQ6r5GZ"
      },
      "outputs": [],
      "source": [
        "# Plot density histogram of the target\n",
        "train[\"target\"].plot(\n",
        "  kind=\"hist\",\n",
        "  title=\"Target\",\n",
        "  figsize=(5, 3),\n",
        "  xlabel=\"Value\",\n",
        "  density=True,\n",
        "  bins=50\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhj9RZGNr5GX"
      },
      "source": [
        "### Features\n",
        "The `features` are quantitative attributes of each stock: fundamentals like P/E ratio, technical signals like RSI, market data like short interest, secondary data like analyst ratings, and much more.\n",
        "\n",
        "The underlying definition of each feature is not important, just know that Numerai has included these features in the dataset because we believe they are predictive of the `target` either by themselves or in combination with other features.\n",
        "\n",
        "Feature values are binned into 5 equal bins: `0`, `1`, `2`, `3`, `4`. This heavy regularization of feature values is to avoid overfitting as the underlying values are extremely noisy. Unlike the target, these are integers instead of floats to reduce the storage needs of the overall dataset.\n",
        "\n",
        "If data for a particular feature is missing for that era (more common in early `eras`), then all values will be set to `2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHlSJccVr5GY"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))\n",
        "first_era = train[train[\"era\"] == train[\"era\"].unique()[0]]\n",
        "last_era = train[train[\"era\"] == train[\"era\"].unique()[-1]]\n",
        "last_era[feature_set[-1]].plot(\n",
        "   title=\"5 equal bins\",\n",
        "   kind=\"hist\",\n",
        "   density=True,\n",
        "   bins=50,\n",
        "   ax=ax1\n",
        ")\n",
        "first_era[feature_set[-1]].plot(\n",
        "   title=\"missing data\",\n",
        "   kind=\"hist\",\n",
        "   density=True,\n",
        "   bins=50,\n",
        "   ax=ax2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eyn-0Or3r5GZ"
      },
      "source": [
        "## 2. Modeling\n",
        "At a high level, our task is to model and predict the `target` using the `features`.\n",
        "\n",
        "### Model training\n",
        "\n",
        "You are free to use any tool or framework, but here we will be using LGBMRegressor, a popular choice amongst tournament participants. While you wait for the model to train, watch this [video](https://www.youtube.com/watch?v=w8Y7hY05z7k) to learn why tree-based models work so well on tabular datasets from our Chief Scientist MDO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prHdeg5Nr5GZ"
      },
      "outputs": [],
      "source": [
        "# https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html\n",
        "import lightgbm as lgb\n",
        "\n",
        "# https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html\n",
        "model = lgb.LGBMRegressor(\n",
        "  n_estimators=2000,\n",
        "  learning_rate=0.01,\n",
        "  max_depth=5,\n",
        "  num_leaves=2**5-1,\n",
        "  colsample_bytree=0.1\n",
        ")\n",
        "# We've found the following \"deep\" parameters perform much better, but they require much more CPU and RAM\n",
        "# model = lgb.LGBMRegressor(\n",
        "#     n_estimators=30_000,\n",
        "#     learning_rate=0.001,\n",
        "#     max_depth=10,\n",
        "#     num_leaves=2**10,\n",
        "#     colsample_bytree=0.1\n",
        "#     min_data_in_leaf=10000,\n",
        "# )\n",
        "\n",
        "# This will take a few minutes 🍵\n",
        "model.fit(\n",
        "  train[feature_set],\n",
        "  train[\"target\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTqgml-br5Ga"
      },
      "source": [
        "### Validation predictions\n",
        "\n",
        "Now let's make some out-of-sample predictions on the validation dataset to evaluate our model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImonnvQYr5Ga"
      },
      "outputs": [],
      "source": [
        "# Download validation data - this will take a few minutes\n",
        "napi.download_dataset(f\"{DATA_VERSION}/validation.parquet\")\n",
        "\n",
        "# Load the validation data and filter for data_type == \"validation\"\n",
        "validation = pd.read_parquet(\n",
        "    f\"{DATA_VERSION}/validation.parquet\",\n",
        "    columns=[\"era\", \"data_type\", \"target\"] + feature_set\n",
        ")\n",
        "validation = validation[validation[\"data_type\"] == \"validation\"]\n",
        "del validation[\"data_type\"]\n",
        "\n",
        "# Downsample to every 4th era to reduce memory usage and speedup evaluation (suggested for Colab free tier)\n",
        "# Comment out the line below to use all the data (slower and higher memory usage, but more accurate evaluation)\n",
        "validation = validation[validation[\"era\"].isin(validation[\"era\"].unique()[::4])]\n",
        "\n",
        "# Eras are 1 week apart, but targets look 20 days (o 4 weeks/eras) into the future,\n",
        "# so we need to \"embargo\" the first 4 eras following our last train era to avoid \"data leakage\"\n",
        "last_train_era = int(train[\"era\"].unique()[-1])\n",
        "eras_to_embargo = [str(era).zfill(4) for era in [last_train_era + i for i in range(4)]]\n",
        "validation = validation[~validation[\"era\"].isin(eras_to_embargo)]\n",
        "\n",
        "# Generate predictions against the out-of-sample validation features\n",
        "# This will take a few minutes 🍵\n",
        "validation[\"prediction\"] = model.predict(validation[feature_set])\n",
        "validation[[\"era\", \"prediction\", \"target\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toGRSHN9r5Ga"
      },
      "source": [
        "### Performance evaluation\n",
        "\n",
        "Numerai calculates scores designed to \"align incentives\" between your model and the hedge fund - a model with good scores should help the hedge fund make good returns. The primary scoring metrics in Numerai are:\n",
        "\n",
        "- `CORR` (or \"Correlation\") which is calculated by the function `numerai_corr` - a Numerai specific variant of the Pearson Correlation between your model and the target.\n",
        "\n",
        "- `MMC` (or \"Meta Model Contribution\") which is a calculated by the function `correlation_contribution` - a measure of how uniquely additive your model is to the Numerai Meta Model.\n",
        "\n",
        "On the Numerai website you will see `CORR` referred to as `CORR20V2`, where the \"20\" refers to the 20-day return target and \"v2\" specifies that we are using the 2nd version of the scoring function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTdo3r_Kr5Ga"
      },
      "outputs": [],
      "source": [
        "# install Numerai's open-source scoring tools\n",
        "!pip install -q --no-deps numerai-tools\n",
        "\n",
        "# import the 2 scoring functions\n",
        "from numerai_tools.scoring import numerai_corr, correlation_contribution\n",
        "\n",
        "# Download and join in the meta_model for the validation eras\n",
        "napi.download_dataset(f\"v4.3/meta_model.parquet\", round_num=842)\n",
        "validation[\"meta_model\"] = pd.read_parquet(\n",
        "    f\"v4.3/meta_model.parquet\"\n",
        ")[\"numerai_meta_model\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BX49Z_Lnr5Gb"
      },
      "source": [
        "As mentioned above, it is important for us to score each historical `era` independantly. So when evaluating the performance of our model, we should be looking at the \"per era\" metrics.\n",
        "\n",
        "One thing you may notice here is how low the scores are (in the range of +/- 0.05). This is very normal in the domain of quantitative finance and is part of the reason why we say Numerai is the \"hardest data science tournament\" in the world."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_qnP9QVr5Gb"
      },
      "outputs": [],
      "source": [
        "# Compute the per-era corr between our predictions and the target values\n",
        "per_era_corr = validation.groupby(\"era\").apply(\n",
        "    lambda x: numerai_corr(x[[\"prediction\"]].dropna(), x[\"target\"].dropna())\n",
        ")\n",
        "\n",
        "# Compute the per-era mmc between our predictions, the meta model, and the target values\n",
        "per_era_mmc = validation.dropna().groupby(\"era\").apply(\n",
        "    lambda x: correlation_contribution(x[[\"prediction\"]], x[\"meta_model\"], x[\"target\"])\n",
        ")\n",
        "\n",
        "\n",
        "# Plot the per-era correlation\n",
        "per_era_corr.plot(\n",
        "  title=\"Validation CORR\",\n",
        "  kind=\"bar\",\n",
        "  figsize=(8, 4),\n",
        "  xticks=[],\n",
        "  legend=False,\n",
        "  snap=False\n",
        ")\n",
        "per_era_mmc.plot(\n",
        "  title=\"Validation MMC\",\n",
        "  kind=\"bar\",\n",
        "  figsize=(8, 4),\n",
        "  xticks=[],\n",
        "  legend=False,\n",
        "  snap=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAMEtbTFr5Gb"
      },
      "source": [
        "Instead of looking at the raw score for each era, it is helpful to look at the cumulative scores.\n",
        "\n",
        "If you are familiar with \"backtesting\" in quant finance where people simulate the historical performance of their investment strategies, you can roughly think of this plot as a backtest of your model performance over the historical validation period.\n",
        "\n",
        "Notice a few things below:\n",
        "\n",
        "- CORR gradually increases over many eras of the validation data even with this simple model on modern data.\n",
        "\n",
        "- MMC is generated over a smaller set of recent eras - this is because the validation time range pre-dates the Meta Model.\n",
        "\n",
        "- MMC is very high early on in the Meta Model's existence, MMC - this is because the newest datasets were not available and models trained on the newest data are could have been very additive in the past.\n",
        "\n",
        "- MMC is flat and decreasing recently because the Meta Model has started catching up to modern data sets and getting correlation has been difficult in recent eras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T62k0nGpr5Gb"
      },
      "outputs": [],
      "source": [
        "# Plot the cumulative per-era correlation\n",
        "per_era_corr.cumsum().plot(\n",
        "  title=\"Cumulative Validation CORR\",\n",
        "  kind=\"line\",\n",
        "  figsize=(8, 4),\n",
        "  legend=False\n",
        ")\n",
        "per_era_mmc.cumsum().plot(\n",
        "  title=\"Cumulative Validation MMC\",\n",
        "  kind=\"line\",\n",
        "  figsize=(8, 4),\n",
        "  legend=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtW4n1jpr5Gc"
      },
      "source": [
        "### Performance metrics\n",
        "\n",
        "To evaluate the performance of our model, it is also helpful to compute some summary metrics over the entire validation period:\n",
        "\n",
        "- `Mean` is the primary measure of your model's long-term performance.\n",
        "\n",
        "- `Sharpe` is a measure of your model's consistency. In finance, the Sharpe ratio of an investment strategy measures risk adjusted returns. In Numerai, we compute sharpe as the mean divided by the standard deviation.\n",
        "\n",
        "- `Max drawdown` is a measure of your model's risk. In finance, the max drawdown of an investment strategy is the largest loss suffered. In Numerai, we compute max drawdown as the maximum peak to trough drop in a cumulative score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EXHy3M0r5Gc"
      },
      "outputs": [],
      "source": [
        "# Compute performance metrics\n",
        "corr_mean = per_era_corr.mean()\n",
        "corr_std = per_era_corr.std(ddof=0)\n",
        "corr_sharpe = corr_mean / corr_std\n",
        "corr_max_drawdown = (per_era_corr.cumsum().expanding(min_periods=1).max() - per_era_corr.cumsum()).max()\n",
        "\n",
        "mmc_mean = per_era_mmc.mean()\n",
        "mmc_std = per_era_mmc.std(ddof=0)\n",
        "mmc_sharpe = mmc_mean / mmc_std\n",
        "mmc_max_drawdown = (per_era_mmc.cumsum().expanding(min_periods=1).max() - per_era_mmc.cumsum()).max()\n",
        "\n",
        "pd.DataFrame({\n",
        "    \"mean\": [corr_mean, mmc_mean],\n",
        "    \"std\": [corr_std, mmc_std],\n",
        "    \"sharpe\": [corr_sharpe, mmc_sharpe],\n",
        "    \"max_drawdown\": [corr_max_drawdown, mmc_max_drawdown]\n",
        "}, index=[\"CORR\", \"MMC\"]).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7k973wHgr5Gc"
      },
      "source": [
        "These performance metrics above are not amazing but that's ok, we are just getting started. In the next few tutorials, you will learn how to improve our model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gC06mZ-r5Gc"
      },
      "source": [
        "## 3. Submissions\n",
        "\n",
        "Unlike Kaggle competitions that evalute models based on <ins>test</ins> performance, Numerai evaluates models based based on <ins>live</ins> performance.\n",
        "\n",
        "### Live predictions\n",
        "\n",
        "Every Tuesday-Saturday, new `live features` are released, which represent the current state of the stock market.\n",
        "\n",
        "Your task is to generate `live predictions` on the unknown target values, which represent stock market returns 20 days into the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUEWmrdnr5Gc"
      },
      "outputs": [],
      "source": [
        "# Download latest live features\n",
        "napi.download_dataset(f\"{DATA_VERSION}/live.parquet\")\n",
        "\n",
        "# Load live features\n",
        "live_features = pd.read_parquet(f\"{DATA_VERSION}/live.parquet\", columns=feature_set)\n",
        "\n",
        "# Generate live predictions\n",
        "live_predictions = model.predict(live_features[feature_set])\n",
        "\n",
        "# Format submission\n",
        "pd.Series(live_predictions, index=live_features.index).to_frame(\"prediction\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAsl2z7mr5Gd"
      },
      "source": [
        "### Model upload\n",
        "\n",
        "To participate in the tournament, you must submit live predictions every Tuesday-Saturday.\n",
        "\n",
        "To automate this process, you can simply:\n",
        "- Define your prediction pipeline as a function\n",
        "- Serialize your function using the `cloudpickle` library\n",
        "- Upload your model pickle file to Numerai\n",
        "- Let Numerai run your model to submit live predictions every day\n",
        "\n",
        "Read more about Model Uploads and other self-hosted automation options in our [docs](https://docs.numer.ai/numerai-tournament/submissions#automation).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4bHP_BGr5Gd"
      },
      "outputs": [],
      "source": [
        "# Define your prediction pipeline as a function\n",
        "def predict(live_features: pd.DataFrame) -> pd.DataFrame:\n",
        "    live_predictions = model.predict(live_features[feature_set])\n",
        "    submission = pd.Series(live_predictions, index=live_features.index)\n",
        "    return submission.to_frame(\"prediction\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l442TN44r5Gd"
      },
      "outputs": [],
      "source": [
        "# Use the cloudpickle library to serialize your function\n",
        "import cloudpickle\n",
        "p = cloudpickle.dumps(predict)\n",
        "with open(\"hello_numerai.pkl\", \"wb\") as f:\n",
        "    f.write(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFUq_XpDr5Gd"
      },
      "outputs": [],
      "source": [
        "# Download file if running in Google Colab\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download('hello_numerai.pkl')\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iplRaPPLr5Gd"
      },
      "source": [
        "That's it! You now have a pickle file that is ready for upload.\n",
        "\n",
        "Head back to the [Hello Numerai Tutorial](https://numer.ai/tutorial/hello-numerai) to upload your model!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}